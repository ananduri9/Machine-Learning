{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Net (RNN)\n",
    "Now we will build a (time-series) recurrent neural net to do sentiment classification with the IMDB Movie reviews dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "First, let's import everything we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM, RNN, GRU\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Embedding, BatchNormalization\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Now let's load the mnist data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape is:  (25000,)\n",
      "y_train shape is:  (25000,)\n",
      "x_test shape is:  (25000,)\n",
      "y_test shape is:  (25000,)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "max_idx = 10000\n",
    "\n",
    "# Constrain dataset to only select top 10000 words\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_idx)\n",
    "\n",
    "print(\"x_train shape is: \", x_train.shape)\n",
    "print(\"y_train shape is: \", y_train.shape)\n",
    "\n",
    "print(\"x_test shape is: \", x_test.shape)\n",
    "print(\"y_test shape is: \", y_test.shape)\n",
    "\n",
    "\n",
    "#save this for later\n",
    "labels = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first take a quick look at what our data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train set: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "The length of this review is:  218\n",
      "Y_train set: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train set:\", x_train[0])\n",
    "print(\"The length of this review is: \", len(x_train[0]))\n",
    "print(\"Y_train set:\", y_train [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the features training set contains reviews encoded as a sequences of integers (indexed by frequency in the data set), and the label is a '1' for a positive review, and '0' for a negative review.\n",
    "\n",
    "Because of this, we will later add an embedding layer that maps each word to an \"embedded\" vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Data\n",
    "In order to feed the data into a neural network, we must make sure that all training and testing examples are the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new x_train shape is:  (25000, 400)\n",
      "new x_test shape is:  (25000, 400)\n"
     ]
    }
   ],
   "source": [
    "# Turn all training examples to length of 400\n",
    "max_len = 400\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, max_len)\n",
    "x_test = sequence.pad_sequences(x_test, max_len)\n",
    "\n",
    "print(\"new x_train shape is: \", x_train.shape)\n",
    "print(\"new x_test shape is: \", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "Now we can build our RNN/LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = Sequential()\n",
    "\n",
    "# Add embedding to words\n",
    "model.add(Embedding(input_dim=max_idx+1, output_dim=64,\n",
    "                    input_length=max_len))\n",
    "\n",
    "# Add 1D Convolutional layer\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Dropout(.15))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Add bidirectional LSTM layer\n",
    "model.add(Bidirectional(LSTM(16, return_sequences=False, recurrent_dropout=.15)))\n",
    "model.add(Dropout(.15))\n",
    "\n",
    "# Add time-ordered dense layer\n",
    "model.add(Dense(32))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "# Add final softmax layer\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a quick look at our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 400, 64)           640064    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 400, 32)           6176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 200, 32)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_18 (Bidirectio (None, 32)                6272      \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 654,945\n",
      "Trainable params: 654,817\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "Now let's fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 86s 3ms/step - loss: 0.4924 - acc: 0.7405 - val_loss: 0.3500 - val_acc: 0.8442\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 82s 3ms/step - loss: 0.2778 - acc: 0.8880 - val_loss: 0.3297 - val_acc: 0.8657\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 85s 3ms/step - loss: 0.1779 - acc: 0.9326 - val_loss: 0.3775 - val_acc: 0.8373\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 84s 3ms/step - loss: 0.1082 - acc: 0.9619 - val_loss: 0.4378 - val_acc: 0.8385\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 84s 3ms/step - loss: 0.0551 - acc: 0.9812 - val_loss: 0.5160 - val_acc: 0.8436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xadd9f9e8>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=128,\n",
    "          epochs=5, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "Let's see how well our model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 26s 1ms/step\n",
      "Test score: 0.5160399737548829\n",
      "Test accuracy: 0.84356\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Test score:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, we get pretty solid results (though it does look like this model is overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to Regular Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this would compare to a regular Dense neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "model = Sequential()\n",
    "\n",
    "# Create first dense layer\n",
    "model.add(Dense(512, input_shape=(400,)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Create second dense layer\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Create third dense layer\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Create final sigmoid layer\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 7s 287us/step - loss: 0.7414 - acc: 0.5159 - val_loss: 0.7645 - val_acc: 0.5093\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 7s 263us/step - loss: 0.6776 - acc: 0.5754 - val_loss: 0.7074 - val_acc: 0.5127\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 7s 263us/step - loss: 0.6340 - acc: 0.6305 - val_loss: 0.7288 - val_acc: 0.5123\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 7s 266us/step - loss: 0.5631 - acc: 0.7006 - val_loss: 0.7868 - val_acc: 0.5153\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 7s 260us/step - loss: 0.4717 - acc: 0.7652 - val_loss: 0.9145 - val_acc: 0.5108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10b47240>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=128,\n",
    "          epochs=5, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 3s 120us/step\n",
      "Test score: 0.9144534546279908\n",
      "Test accuracy: 0.5108\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Test score:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow - what a difference between a vanilla neural net and the recurrent neural net. The regular neural net barely yields an accuracy over 50%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
